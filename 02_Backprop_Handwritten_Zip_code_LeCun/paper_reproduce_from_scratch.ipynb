{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15393dfb",
   "metadata": {},
   "source": [
    "## WARNING\n",
    "I tried to implement from scratch LeCun's neural network. Unfortunately, **I wasn't able to do it precisely** (I suspect the gradient calculation in the first *H1* layer might not be entirely properly implemented). Thus, I moved to PyTorch (in the other notebook) and used the framework to recreate a network, similar (but not 100% identical) to LeCun's one.\n",
    "\n",
    "- - - \n",
    "\n",
    "## DEPRECATED\n",
    "\n",
    "NB: You need the latest version of **dmlearn** in order to run current notebook:\n",
    "\n",
    "```pip install dmlearn```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ef99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "from dmlearn.signal import convolve_tensor, correlate_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e385131c",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "### 1.1 Load from HF\n",
    "[USPS](https://huggingface.co/datasets/flwrlabs/usps) is a digit dataset scanned from envelopes by the U.S. Postal Service containing a total of $9,298$ samples of handwritten digits:\n",
    "* $7,291$ digits are used for training\n",
    "* $2,007$ digits are used for testing\n",
    "* each image is $16$ x $16$ pixels grayscale (not binary)\n",
    "* the images are within $[0,255]$ range, but we will normalize it to $[-1,1]$\n",
    "* the images are centered\n",
    "* they show a broad range of font styles\n",
    "\n",
    "One important feature of these images is that both the training set and the testing set contain numerous examples that are ambiguous, unclassifiable, or even misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc478d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bytes': b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      6\n",
       "1  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      5\n",
       "2  {'bytes': b\"\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      4\n",
       "3  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      7\n",
       "4  {'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHD...      3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "train_df = pd.read_parquet(\"hf://datasets/flwrlabs/usps/\" + splits[\"train\"])\n",
    "test_df = pd.read_parquet(\"hf://datasets/flwrlabs/usps/\" + splits[\"test\"])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd398aa",
   "metadata": {},
   "source": [
    "### 1.2 Convert bytes to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0060df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN SET ===\n",
      "train_images shape: (100, 1, 16, 16)\n",
      "train_image_0 shape: (16, 16)\n"
     ]
    }
   ],
   "source": [
    "def convert_image_bytes_to_int(img: dict) -> np.ndarray:\n",
    "    # Convert the image bytes to numpy arrays\n",
    "    image = Image.open(io.BytesIO(img['bytes']))\n",
    "    return np.array(image)\n",
    "\n",
    "def convert_image_df_to_numpy(image_df: pd.DataFrame) -> np.ndarray:\n",
    "    images = image_df['image'].apply(convert_image_bytes_to_int)\n",
    "    # Convert to numpy array (consider the shape of all images is the same - 16x16)\n",
    "    train_images = np.zeros((len(images), 1, 16, 16))\n",
    "    for i, image in enumerate(images):\n",
    "        train_images[i][0] = image\n",
    "    return train_images\n",
    "\n",
    "train_images = convert_image_df_to_numpy(train_df)\n",
    "train_images = train_images[:100] # For testing purposes, use only 100 of all 7291 training samples\n",
    "\n",
    "print(f\"=== TRAIN SET ===\")\n",
    "print(f\"train_images shape: {train_images.shape}\")\n",
    "print(f\"train_image_0 shape: {train_images[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a403cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST SET ===\n",
      "test_images shape: (2007, 1, 16, 16)\n",
      "test_image_0 shape: (16, 16)\n"
     ]
    }
   ],
   "source": [
    "test_images = convert_image_df_to_numpy(test_df)\n",
    "print(f\"=== TEST SET ===\")\n",
    "print(f\"test_images shape: {test_images.shape}\")\n",
    "print(f\"test_image_0 shape: {test_images[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0c99a",
   "metadata": {},
   "source": [
    "## 2. Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff4f0c",
   "metadata": {},
   "source": [
    "### 2.1 Normalize data [-1, 1]\n",
    "\n",
    "In order to prevent exploding gradients and slower computation we will scale the data to smaller numbers. \n",
    "\n",
    "In the original paper the data was scaled to the range of $[-1,1]$. We will do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573f8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data [-1, 1]. We first scale the data to the range of [0, 2] and then shift it to the range of [-1, 1].\n",
    "train_images = train_images / 127.5 - 1\n",
    "test_images = test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f4418",
   "metadata": {},
   "source": [
    "### 2.2 One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5875b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN SET ===\n",
      "train_labels.shape: (7291, 10, 1)\n",
      "train_labels[0]: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "### Convert labels to one-hot encoding\n",
    "def convert_labels_to_one_hot(df: pd.DataFrame) -> np.ndarray:\n",
    "    labels = df['label'].values\n",
    "    labels = np.eye(10)[labels]\n",
    "    return labels\n",
    "\n",
    "train_labels = convert_labels_to_one_hot(train_df)\n",
    "# Transform each label from horizontal to vertical\n",
    "train_labels = train_labels.reshape(train_labels.shape[0], 10, 1)\n",
    "print(f\"=== TRAIN SET ===\")\n",
    "print(f\"train_labels.shape: {train_labels.shape}\")\n",
    "print(f\"train_labels[0]: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bec5e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST SET ===\n",
      "test_labels.shape: (2007, 10)\n",
      "test_labels[0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "test_labels = convert_labels_to_one_hot(test_df)\n",
    "print(f\"=== TEST SET ===\")\n",
    "print(f\"test_labels.shape: {test_labels.shape}\")\n",
    "print(f\"test_labels[0]: {test_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7802d7",
   "metadata": {},
   "source": [
    "## 3. Convolution operation\n",
    "We need to implement an operation that convolves two 3D matrices. We can't directly use the `scipy.signal` library since, it doesn't fully support **stride**, **padding** and **padding_fill_value** configuration.\n",
    "\n",
    "Thus, I have developed a private library that provides full flexibility in regard to these parameters.\n",
    "\n",
    "To install the library run:\n",
    "\n",
    "```\n",
    "pip install dmlearn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c681b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution result:\n",
      "\n",
      "[[[-1 -1 -1]\n",
      "  [-1 -1 -1]\n",
      "  [-1 -1 -1]]\n",
      "\n",
      " [[ 1  2  3]\n",
      "  [ 7  8  9]\n",
      "  [13 14 15]]\n",
      "\n",
      " [[ 1  2  3]\n",
      "  [ 7  8  9]\n",
      "  [13 14 15]]\n",
      "\n",
      " [[-1 -1 -1]\n",
      "  [-1 -1 -1]\n",
      "  [-1 -1 -1]]]\n"
     ]
    }
   ],
   "source": [
    "# Test the function with a 3D matrix and a 2D kernel\n",
    "tensord_3d = np.array([[[1, 2, 2, 3, 3],\n",
    "                    [4, 5, 5, 6, 6],\n",
    "                    [7, 8, 8, 9, 9],\n",
    "                    [10, 11, 11, 12, 12],\n",
    "                    [13, 14, 14, 15, 15]],\n",
    "                   [[1, 2, 2, 3, 3],\n",
    "                    [4, 5, 5, 6, 6],\n",
    "                    [7, 8, 8, 9, 9],\n",
    "                    [10, 11, 11, 12, 12],\n",
    "                    [13, 14, 14, 15, 15]]])\n",
    "\n",
    "# The filter (kernel) to convolve the tensor with\n",
    "kernel = np.array([[0, 0, 0],\n",
    "                   [0, 1, 0],\n",
    "                   [0, 0, 0]])\n",
    "\n",
    "# Convolution parameters\n",
    "stride = 2\n",
    "padding = 1\n",
    "padding_value = -1\n",
    "\n",
    "# Perform the convolution\n",
    "conv_result = convolve_tensor(tensord_3d, kernel, step_size=stride, padding=padding, fill_value=padding_value)\n",
    "print(f\"Convolution result:\\n\\n{conv_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d17a0d",
   "metadata": {},
   "source": [
    "## 4. Neural Net Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5254d",
   "metadata": {},
   "source": [
    "### 4.1 Random weights\n",
    "Weights are initialized with random values within $U[-2.4/F, 2.4/F]$ range, where $F$ is the fan-in (number of inputs, connected to a neuron or a layer). \n",
    "\n",
    "**Reasoning**: *\"tends to keep total inputs in operating range of sigmoid\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4765c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights(shape: tuple, fan_in: int) -> np.ndarray:\n",
    "    min_val = -np.sqrt(2.4 / fan_in)\n",
    "    max_val = np.sqrt(2.4 / fan_in)\n",
    "    return np.random.uniform(min_val, max_val, shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7602b15",
   "metadata": {},
   "source": [
    "### 4.2 Activation function - LiSHT\n",
    "Basically, this function computes linearly scaled hyperbolic tangent:\n",
    "\n",
    "$$\\text{lisht}(x) = x * \\text{tanh}(x)$$\n",
    "\n",
    "#### Why Tanh\n",
    "\n",
    "In this paper LeCun's team applied **scaled hyperbolic tangent** (i.e. tanh) function to the output of each layer in the neural net. So the question is why this function? Why not simple `sigmoid` which is basically the same function, but within the $[0,1]$ range (`tanh`'s range is $[-1, 1]$).\n",
    "\n",
    "The only reason I can come up with is that `tanh` has steeper gradients than sigmoid (due to the bigger range), which means faster learning. Of course, we can achieve faster learning with higher `learning rate`. However, latter would increase the risk of divergence.\n",
    "\n",
    "Read more [HERE >>>](https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function) .\n",
    "\n",
    "LeCun himself wrote following text in another paper:\n",
    "* *Symmetric functions of that kind are believed to yield **faster convergence**, although the learning can be extremely slow if some weights are too small (LeCun 1987).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c48d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lisht(x: np.ndarray) -> np.ndarray:\n",
    "    return x * np.tanh(x)\n",
    "\n",
    "def lisht_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    return np.tanh(x) + x * (1 - np.tanh(x)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72d7b8",
   "metadata": {},
   "source": [
    "### 4.3 Cost function - MSE\n",
    "The output cost function was the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515e29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Mean Squared Error\n",
    "\n",
    "    Args:\n",
    "        y_true: np.ndarray - the true labels - one-hot encoded (e.g. [0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
    "        y_pred: np.ndarray - the predicted labels - one-hot encoded (e.g. [-0.05, -0.5, 0.15, 0.9, 0.25, 0.35, -0.6, 0.1, -0.7, 0])\n",
    "    Returns:\n",
    "        float - the mean squared error\n",
    "    \"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def loss_mse_derivative(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    return 2 * (y_pred - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c386d1",
   "metadata": {},
   "source": [
    "### 4.4 Stochastic gradient descent\n",
    "In this paper SGD is chosen over Batch GD with following argument:\n",
    "* *The weights were updated according to the so-called stochastic gradient or \"on-line\" procedure (updating after each presentation of a single pattern) as opposed to the \"true\" gradient procedure (averaging over the whole training set before updating the weights). From empirical study (supported by theoretical arguments), the stochastic gradient was found to converge much faster than the true gradient, especially on large, redundant data bases. It also finds solutions that are\n",
    "more robust.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e0649",
   "metadata": {},
   "source": [
    "### 4.5 Neural Net Design\n",
    "\n",
    "<center><img src=\"img/lecun_zip_code_nn.png\" alt=\"Neural Network Architecture\" width=\"921\" height=\"468\" /></center>\n",
    "<p style=\"text-align: center; font-size: small;\"><i><b>Figure 1.</b> 1989 LeCun ConvNet per description in the paper</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d41a4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b76bded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self):\n",
    "        self.stride = 2\n",
    "        self.padding_value = -1         # In this paper, the padding value is -1 (the min value in the dataset)\n",
    "        self.num_of_kernels = 12        # In all layers, the number of kernels is 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbe200e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H1(Convolutional):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = (1, 16, 16)                      # In first layer the input is basically a 2D 16x16 image.\n",
    "        self.padding = 2\n",
    "        self.kernel_shape = (self.num_of_kernels, 5, 5)\n",
    "        self.output_shape = (self.num_of_kernels, 8, 8)     # In this paper, the output is always 1/2 of the input\n",
    "\n",
    "        # Generate random weights and biases\n",
    "        # TODO: use Xavier initialization\n",
    "        fan_in = self.input_shape[1] * self.input_shape[2]\n",
    "        self.kernels = random_weights(self.kernel_shape, fan_in)  # (12, 5, 5) - 12 kernels, each with 25 weights (300 params in total)\n",
    "        self.biases = random_weights(self.output_shape, fan_in)   # (12, 8, 8) - each unit has its own bias (768 params in total)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input                  # 1 x 16 x 16\n",
    "        self.output = np.copy(self.biases)  # 12 x 8 x 8\n",
    "        self.logits = np.zeros((self.num_of_kernels, self.output_shape[1], self.output_shape[2]))\n",
    "\n",
    "        # For each kernel (we have 12 of them in this layer)\n",
    "        for i in range(self.num_of_kernels): \n",
    "            # For each depth of the input (in this layer it's 1)\n",
    "            # The formula is:\n",
    "            # - output[i] = input[j] * kernels[i]\n",
    "            # where:\n",
    "            # - output[i] is the output of the i-th kernel\n",
    "            # - input is basically the input image (16 x 16). We ignore the first dimension, since the image it's 1 (black and white image).\n",
    "            # - kernels[i] is the i-th kernel\n",
    "            cur_output = convolve_tensor(self.input[0], self.kernels[i], step_size=self.stride, padding=self.padding, fill_value=self.padding_value)\n",
    "            # We apply LiSHT (X * tanh(X)) in all units of the layer\n",
    "            self.logits[i] = cur_output\n",
    "            self.output[i] = lisht(cur_output)\n",
    "\n",
    "        # Return the output of the layer\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        kernels_gradient = np.zeros(self.kernel_shape) # 12 x 4 x 4\n",
    "        biases_gradient = np.zeros(self.output_shape)  # 12 x 4 x 4\n",
    "        lisht_gradient = np.zeros((self.output_shape[0], self.output_shape[1], self.output_shape[2])) # 12 x 4 x 4\n",
    "\n",
    "        # For each output kernel (we have 12 of them in this layer)\n",
    "        for i in range(self.num_of_kernels):\n",
    "            # Calculate the gradient of the loss with respect to the input of the j-th feature map\n",
    "            lisht_gradient[i] = lisht_derivative(self.logits[i]) * output_gradient[i]\n",
    "            kernels_gradient[i] = correlate_tensor(self.input[0], lisht_gradient[i], step_size=self.stride, padding=0, fill_value=self.padding_value)\n",
    "            biases_gradient[i] = lisht_gradient[i]\n",
    "        \n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * biases_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7ed00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 16)\n",
      "(12, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass of H1\n",
    "h1 = H1()\n",
    "print(train_images[0].shape)\n",
    "h1_output = h1.forward(train_images[0])\n",
    "print(h1_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "356fc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2(Convolutional):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = (12, 8, 8)                       # In second layer the input is 12 times 8x8 feature maps.\n",
    "        self.padding = 1\n",
    "        self.kernel_shape = (self.num_of_kernels, 4, 4)\n",
    "        self.output_shape = (self.num_of_kernels, 4, 4)     # In this paper, the output is always 1/2 of the input\n",
    "        \n",
    "        # Generate random weights and biases\n",
    "        # TODO: use Xavier initialization\n",
    "        fan_in = self.input_shape[0] * self.input_shape[1] * self.input_shape[2]\n",
    "        self.kernels = random_weights(self.kernel_shape, fan_in)  # (12, 4, 4) - 12 kernels, each with 16 weights (192 params in total)\n",
    "        self.biases = random_weights(self.output_shape, fan_in)   # (12, 4, 4) - each unit has its own bias (192 params in total)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input                  # 12 x 8 x 8\n",
    "        self.output = np.copy(self.biases)  # 12 x 4 x 4\n",
    "        self.logits = np.zeros((self.num_of_kernels, 12, self.output_shape[1], self.output_shape[2]))\n",
    "\n",
    "        # For each output kernel (we have 12 of them in this layer)\n",
    "        for i in range(self.num_of_kernels):\n",
    "            # NB: We pick only 8 out of 12 input feature maps. We want for each `i` the picked 8 input feature maps to be different.\n",
    "            # We pick the 8 input feature maps by using the `i` index.\n",
    "            # For example, if `i = 0`, we pick the first 8 input feature maps.\n",
    "            # If `i = 1`, we pick the second 8 input feature maps.\n",
    "            # And so on.\n",
    "            # We do this because we want the picked 8 input feature maps to be different for each kernel.\n",
    "            # This is important because we want the picked 8 input feature maps to be different for each kernel.\n",
    "            # In the paper is written: \"The eight maps in H1 on which a map in H2 takes its inputs are chosen according a scheme that will not be described here\" :)\n",
    "            feature_maps_range = (np.array([0, 1, 2, 3, 4, 5, 6, 7]) + i) % 12\n",
    "            # For each of the 8 input feature maps we calculate the convolution with the i-th kernel.\n",
    "            for j in feature_maps_range:\n",
    "                # The formula is:\n",
    "                # - output[i] += input[j] * kernels[i]\n",
    "                # where:\n",
    "                # - output[i] is the output of the i-th kernel\n",
    "                # - input[j] is basically a feature map [8x8]. We ignore the first dimension, since the image it's 1 (black and white image).\n",
    "                # - kernels[i] is the i-th kernel\n",
    "                cur_output = convolve_tensor(self.input[j], self.kernels[i], step_size=self.stride, padding=self.padding, fill_value=self.padding_value)\n",
    "                # We apply LiSHT (X * tanh(X)) in all units of the layer\n",
    "                self.logits[i, j] = cur_output\n",
    "                self.output[i] += lisht(cur_output)  # TODO: should we have output[i][j] = cur_output?\n",
    "\n",
    "        # Return the output of the layer\n",
    "        # Output = bias + sum(input * kernel)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        kernels_gradient = np.zeros(self.kernel_shape) # 12 x 4 x 4\n",
    "        biases_gradient = np.zeros(self.output_shape)  # 12 x 4 x 4\n",
    "        input_gradient = np.zeros(self.input_shape)    # 12 x 8 x 8\n",
    "        lisht_gradient = np.zeros((self.input_shape[0], self.output_shape[0], self.output_shape[1], self.output_shape[2])) # 12 x 12 x 4 x 4\n",
    "\n",
    "        # For each output kernel (we have 12 of them in this layer)\n",
    "        for i in range(self.num_of_kernels):\n",
    "            # Determine the feature maps that were used to calculate the output of the i-th kernel\n",
    "            feature_maps_range = (np.array([0, 1, 2, 3, 4, 5, 6, 7]) + i) % 12\n",
    "            dj_dk_i = np.zeros((12, self.output_shape[1], self.output_shape[2])) # 12 x 4 x 4\n",
    "            input_grads = np.zeros((12, 8, 8))\n",
    "            for j in feature_maps_range:\n",
    "                # Calculate the gradient of the loss with respect to the input of the j-th feature map\n",
    "                lisht_gradient[i][j] = lisht_derivative(self.logits[i, j]) * output_gradient[i]\n",
    "                dj_dk_i[j] = correlate_tensor(self.input[j], lisht_gradient[i][j], step_size=self.stride, padding=self.padding, fill_value=self.padding_value)\n",
    "                input_grads[j] = convolve_tensor(lisht_gradient[i][j], self.kernels[i], step_size=self.stride, padding=7, fill_value=self.padding_value)\n",
    "            kernels_gradient[i] = np.sum(dj_dk_i, axis=0)\n",
    "            biases_gradient[i] = np.sum(lisht_gradient[i], axis=0)\n",
    "            input_gradient[i] = np.sum(input_grads, axis=0)\n",
    "        \n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * biases_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "011f9c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass of H2\n",
    "h2 = H2()\n",
    "h2_output = h2.forward(h1_output)\n",
    "print(h2_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070716d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "976f2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3(Dense):\n",
    "    \"\"\"\n",
    "    Dense (fully-connected) layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize random weights and biases\n",
    "        fan_in = 12 * 4 * 4\n",
    "        self.weights = random_weights((30, fan_in), fan_in) # 30 units in the output layer, 192*30 parameters in total\n",
    "        self.bias = random_weights((30, 1), fan_in)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input  # 12 x 4 x 4\n",
    "        # Return the standard output of the layer (weights * input + bias)\n",
    "        self.logits = np.dot(self.weights, self.input.reshape(12*4*4, 1)) + self.bias\n",
    "        # We apply LiSHT (X * tanh(X)) in all units of the layer\n",
    "        return lisht(self.logits)\n",
    "\n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        # Firstly, we need to calculate the derivative of the activation function.\n",
    "        lisht_gradient = lisht_derivative(self.logits) * output_gradient\n",
    "        # Calculate the weights gradient.\n",
    "        # NB: We need to reshape the input from (12x4x4) to (192, 1). In other words, we need to flatten the input.\n",
    "        # (30, 192) = (30, 1) x (1, 192)\n",
    "        weights_gradient = np.dot(lisht_gradient, self.input.reshape(12*4*4, 1).T)\n",
    "        # Calculate the input gradient\n",
    "        input_gradient = np.dot(self.weights.T, lisht_gradient)\n",
    "        # COnvert (192, 1) to (12, 4, 4)\n",
    "        input_gradient = input_gradient.reshape(12, 4, 4)\n",
    "        # Update the weights and bias\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * lisht_gradient\n",
    "        return input_gradient  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "583340c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass of H2\n",
    "h3 = H3()\n",
    "h3_output = h3.forward(h2_output)\n",
    "print(h3_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db6c125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class H4(Dense):\n",
    "    \"\"\"\n",
    "    Dense (fully-connected) layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize random weights and biases\n",
    "        fan_in = 30\n",
    "        self.weights = random_weights((10, fan_in), fan_in) # 10 units in the output layer, 30*10 parameters in total\n",
    "        self.bias = random_weights((10, 1), fan_in)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.input = input  # 30 x 1\n",
    "        # Return the standard output of the layer (weights * input + bias)\n",
    "        self.logits = np.dot(self.weights, self.input) + self.bias\n",
    "        # We apply LiSHT (X * tanh(X)) in all units of the layer\n",
    "        return lisht(self.logits)\n",
    "    \n",
    "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
    "        # Firstly, we need to calculate the derivative of the activation function.\n",
    "        lisht_gradient = lisht_derivative(self.logits) * output_gradient\n",
    "        # Calculate the weights gradient.\n",
    "        weights_gradient = np.dot(lisht_gradient, self.input.T)\n",
    "        # Calculate the input gradient\n",
    "        input_gradient = np.dot(self.weights.T, lisht_gradient)\n",
    "        # Update the weights and bias\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * lisht_gradient\n",
    "        return input_gradient  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5231b72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass of H2\n",
    "h4 = H4()\n",
    "h4_output = h4.forward(h3_output)\n",
    "print(h4_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70872b",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "577fd5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/23, error=0.09558457757540668\n",
      "2/23, error=0.09551593776192796\n",
      "3/23, error=0.09539353001158467\n",
      "4/23, error=0.0952071945309306\n",
      "5/23, error=0.09494942600589401\n",
      "6/23, error=0.09475451194296466\n",
      "7/23, error=0.09510345685433641\n",
      "8/23, error=0.09674857277936397\n",
      "9/23, error=0.0998480705358504\n",
      "10/23, error=0.10313045136695857\n",
      "11/23, error=0.10515618621721469\n",
      "12/23, error=0.10608429436253167\n",
      "13/23, error=0.10736460147461813\n",
      "14/23, error=0.110686745381784\n",
      "15/23, error=0.11802939773811166\n",
      "16/23, error=0.13307270140649197\n",
      "17/23, error=0.16118485431225263\n",
      "18/23, error=0.20140218516166913\n",
      "19/23, error=0.24539093375950485\n",
      "20/23, error=0.27744153934643007\n",
      "21/23, error=0.3169795618391953\n",
      "22/23, error=0.34752027708900074\n",
      "23/23, error=0.3527431798950319\n"
     ]
    }
   ],
   "source": [
    "def train(network, x_train, y_train, epochs = 23, learning_rate = 1.e-5, verbose = True):\n",
    "    error_history = []\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            output = x\n",
    "            # forward\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "\n",
    "            # error\n",
    "            error += loss_mse(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_mse_derivative(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        error_history.append(error)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")\n",
    "    return error_history\n",
    "\n",
    "network = [H1(), H2(), H3(), H4()]\n",
    "error_history = train(network, train_images, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
